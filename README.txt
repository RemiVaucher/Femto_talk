
Some Notebook to understand the coding of Neural Networks and ScikitLearn from the MASPIN Talk

- The "ScikitLearn" folder: incoming.
- The "Neural Networks understanding" folder: With a huge thank you to Benoit Fabrèges (http://math.univ-lyon1.fr/~fabreges/homepage/) who let me use these pedagogically   beautiful notebooks. You can find:
  1 Problems: a short (but complet) explanation of the two main problems (classification and regression)
  2 Loss Functions: some examples of loss functions
  3 Optimization: a review of some optimizers (SGD, Adam, etc.) and a nice visualisation in 2D.
  4 Automatic Differentiation: WARNING! Some powerful mathematics detected! The explanation behind the automatic gradient calculus of tensorflow (the hidden complex part)
  5 Tensorflow: First, we understand Tensors objects. Then we understand the construction of a Neural Networks with tensorflow. And finally, how to prepare data for feeding the neural network.
  6 Examples/Exercise: An example of classification using neural networks on MNIST, and an exercise of regression using neural networks. A correction will come soon.
  7 More complex Neural Networks: incoming. Teaser, when you want to create something more complex, as UNet, or with many inputs/outputs, the building of your Neural Network is not the same. A quick review of the functional aspect of Tensorflow
  8 Using a GPU - hello PyTorch my old friend: A quick review of PyTorch (and then you can chose between Tensorflow and PyTorch) Incoming soon

- The "Bonus" folder: Again, with all my gratitude to Benoit Fabrèges and Frédérique Lagoutière who led us through this exercise, I present you some Notebook to realize a numerical scheme for EDP resolving using Neural Networks. Incoming soon in french, and a bit later in english.
