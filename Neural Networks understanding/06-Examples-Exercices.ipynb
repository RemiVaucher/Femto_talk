{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification\n",
    "\n",
    "We want to train a neural network on the **MNIST** database and compare the results with the one obtained by using the kNN method.\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "1. Load the MNIST training and test datasets using tensorflow_datasets\n",
    "2. Define the Neural Network\n",
    "3. Define a loss function and an optimizer\n",
    "4. Prepare and optimize train dataset\n",
    "5. Train the network on the training data\n",
    "6. Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display one image\n",
    "def show_image(ax, image, title=None):\n",
    "    ax.imshow(image, cmap='Greys')\n",
    "    ax.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "\n",
    "# Display some images from the given dataset\n",
    "def show_mnist(dataset, nrow, ncol):\n",
    "    plt.figure(figsize=(1.2*ncol, 1.4*nrow))\n",
    "    for i, (image, label) in dataset.enumerate():\n",
    "        ax = plt.subplot(int(nrow), int(ncol), int(i+1))\n",
    "        show_image(ax, image, title=f\"This is a {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full dataset\n",
    "We first apply these steps by optimizing the model on the full dataset at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading MNIST traning and testing dataset\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "(train_dataset, test_dataset), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = ds_info.splits['train'].num_examples\n",
    "ntest = ds_info.splits['test'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Examples of data\n",
    "show_mnist(train_dataset.shuffle(ntrain).take(3*8), 3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Rescaling, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the model\n",
    "\n",
    "# Images are 28x28 pixels. This is the number of inputs.\n",
    "img_height = ds_info.features['image'].shape[0]\n",
    "img_width = ds_info.features['image'].shape[1]\n",
    "n_out = ds_info.features['label'].num_classes\n",
    "\n",
    "# Feed-forward neural network\n",
    "model = Sequential()\n",
    "model.add(Rescaling(1./255, input_shape=(img_height, img_width, 1)))   # Rescaling image data ([0, 255] -> [0, 1])\n",
    "model.add(Flatten())                                                   # From 2D image to 1D\n",
    "model.add(Dense(img_height*img_width, activation='relu'))  # Dense matrix of size 28x28 with ReLU activation layer\n",
    "model.add(Dense(n_out, activation='linear'))               # Dense matrix of size 28x10 with linear activation layer\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.metrics import SparseCategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define a loss function and an optimizer\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate=1e-3),\n",
    "              loss = SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[SparseCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prepare train dataset\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "dataset = train_dataset.cache()\n",
    "dataset = dataset.shuffle(ntrain, reshuffle_each_iteration=True)\n",
    "dataset = dataset.batch(batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train the neural network\n",
    "\n",
    "epochs = 10\n",
    "model.fit(dataset, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Evaluate the model on test dataset\n",
    "\n",
    "testset = test_dataset.batch(batch_size).prefetch(128)\n",
    "model.evaluate(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red & blue dots classification\n",
    "\n",
    "Testing the circle configuration using [TensorFlow playground](https://playground.tensorflow.org/):\n",
    "- perceptron\n",
    "- consecutive linear layers (with linear activation)\n",
    "- 2 hidden layers of 4 and 2 neurons\n",
    "- ReLU vs tanh\n",
    "- perceptron with augmented input dimension ($x^2$ and $y^2$) $\\Rightarrow$ **linearly separable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating a function\n",
    "\n",
    "Given a function $f: \\mathbb{R}^N \\to \\mathbb{R}^M$, it is possible to approximate it by a neural network with $N$ inputs and $M$ outputs, trained on a sample of couples $(x, f(x))$.\n",
    "\n",
    "What kind of function can we approximate ?\n",
    "\n",
    "For a **1-layer** neural network (input directly connected to the output through a linear layer), we can only well approximate a **linear function**.\n",
    "\n",
    "But with a **2-layer** neural network with a \"sigmoid-like\" hidden activation layer, we can approximate arbitrarily well **any continuous function** $f: \\mathbb{R}^N \\to \\mathbb{R}^M$, provided many sufficient units in the hidden layer (see https://www.sciencedirect.com/science/article/abs/pii/0893608089900208 and https://link.springer.com/article/10.1007/BF02551274).\n",
    "\n",
    "Take a look at http://cs231n.github.io/neural-networks-1/#power and https://calcul.math.cnrs.fr/attachments/evt_sci/2019-05-mini-symposium-smai/smai_2019_gribonval.pdf to learn more about the **approximation space** of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\mathbb{R}$ to $\\mathbb{R}$ functions\n",
    "\n",
    "We want to fit a set of points $(x_k, y_k)$ by a model $m_{\\theta}$, where $\\theta$ are parameters to estimate. The function we want to minimize is the following:\n",
    "$$\n",
    "J(\\mathbf{\\theta}) = \\frac{1}{2 K} \\sum_{k=1}^K \\left(m_{\\theta}(x_k) - y_k\\right)^2 .\n",
    "$$\n",
    "\n",
    "This function is the loss function. Note that a **2-layer** neural network model $m$ with $k$ neurons in the hidden layer and the activation function $\\varphi$ can be written as:\n",
    "\n",
    "$$\n",
    "m(x) = \\sum_{i=1}^k c_i \\varphi(a_i x + b_i) + d_i .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function f that you want to approximate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array x containing 50 points in your domain ([-1, 1] for example)\n",
    "\n",
    "# Apply your function f to get an array y. Add some noise to the data.\n",
    "# (add a random number drawn from a gaussian distribution to each element of y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the function f and the points (x_k, y_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training dataset from the numpy arrays x and y. \n",
    "# x are the training data\n",
    "# y are the labels\n",
    "\n",
    "\n",
    "# cache, shuffle, batch and prefetch the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential neural network with some Dense layers (2 or 3).\n",
    "# This neural network takes input of size 1 (one real number x_k) and returns a scalar value (one real number y_k)\n",
    "# Use the activation function of your choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimizer and a loss function for your model.\n",
    "# The loss function is given at the beginning of this part and already exists in Tensorflow (not a custom loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model (size of batches and number of epochs are up to you)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some test data x_test and y_test\n",
    "\n",
    "# Evaluate your model with these test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results :\n",
    "# Create an array of evenly spaced (x_i) for plotting\n",
    "# Apply your model with these points x_i and plot the curve of your model (x_i, y_i)\n",
    "# Plot the 50 points (x_k, y_k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
