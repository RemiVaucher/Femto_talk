{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization algorithms\n",
    "\n",
    "The goal here is to review some of the algorithms used to minimize a loss function in the context of machine learning.\n",
    "\n",
    "We consider the minimization of a sum of [Rosenbrock functions](https://en.wikipedia.org/wiki/Rosenbrock_function):\n",
    "\n",
    "$$ \\textrm{loss}(x, y) = \\frac{1}{N} \\sum_{i=1}^N f_i(x, y)$$\n",
    "\n",
    "with\n",
    "\n",
    "$$ f_i(x, y) = (a_i - x)^2 + b_i (y - x^2)^2$$\n",
    "\n",
    "These functions have two parameters, $a_i$ and $b_i$, that we choose randomly around some values.\n",
    "\n",
    "To use gradient-based minimization algorithms, we also need to define the **gradient** of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N Rosenbrock functions\n",
    "N = 1000\n",
    "\n",
    "# The two parameters for each Rosenbrock function\n",
    "np.random.seed(0)\n",
    "a = 0.1 * np.random.randn(N) + 1\n",
    "b = 1 * np.random.randn(N) + 10\n",
    "\n",
    "# This is the minimum of the sum of the Rosenbrock functions\n",
    "ref_min_x = np.mean(a)\n",
    "ref_min_y = ref_min_x**2\n",
    "\n",
    "# The Rosenbrock function\n",
    "def f(a, b, x, y):\n",
    "    return (x - a)**2 + b * (x**2 - y)**2\n",
    "\n",
    "# Its gradient\n",
    "def dx_f(a, b, x, y):\n",
    "    return 2 * (x - a) + 4 * b * (x**2 - y) * x\n",
    "\n",
    "def dy_f(a, b, x, y):\n",
    "    return -2 * b * (x**2 - y)\n",
    "\n",
    "def grad_f(a, b, x, y):\n",
    "    return np.array([dx_f(a, b, x, y), dy_f(a, b, x, y)])\n",
    "\n",
    "# Loss function, the one we want to minimize\n",
    "# Sum of Rosenbrock functions\n",
    "def loss(x, y):\n",
    "    res = np.zeros(x.shape)\n",
    "    for i in range(N):\n",
    "         res += f(a[i], b[i], x, y) / N\n",
    "    return res\n",
    "\n",
    "# Its gradient\n",
    "def grad_loss(x, y):\n",
    "    res = np.zeros((x.size, 2))\n",
    "    for i in range(N):\n",
    "         res += grad_f(a[i], b[i], x, y) / N\n",
    "    return res\n",
    "\n",
    "# Its gradient selecting only a batch of Rosenbrock function (not the full sum)\n",
    "def grad_loss_batch(I, x, y):\n",
    "    res = np.zeros((x.size, 2))\n",
    "    for i in I:\n",
    "        res += grad_f(a[i], b[i], x, y)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Displaying the loss function\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-2, 2, 0.01)\n",
    "Y = np.arange(-1, 3, 0.01)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = loss(X, Y)\n",
    "\n",
    "# The figure\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "\n",
    "# First subplot (3d plot)\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False, alpha=0.8)\n",
    "ax1.scatter(ref_min_x, ref_min_y, loss(ref_min_x, ref_min_y), color='red')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "\n",
    "# Second subplot (contour plot)\n",
    "levels = [1e-5, 1e-1, 1, 10, 50, 100, 200]\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.contour(X, Y, Z, levels=levels, cmap=cm.coolwarm, vmin=np.amin(Z), vmax=np.amax(Z))\n",
    "ax2.plot(ref_min_x, ref_min_y, \"r*\")\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "\n",
    "cb = plt.colorbar(surf, ax=[ax1, ax2], orientation=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The start point for the minimization\n",
    "start_point = np.array([0.1, 2.5])\n",
    "\n",
    "# nstep for each algorithms (easier to compare)\n",
    "nstep = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to play the animation of the results\n",
    "def plot_method(*args):\n",
    "    fig = plt.figure()\n",
    "    plt.contour(X, Y, Z, levels=levels, cmap=cm.coolwarm, vmin=np.amin(Z), vmax=np.amax(Z), alpha=0.5)\n",
    "    plt.plot(start_point[0], start_point[1], \"rX\")\n",
    "    plt.plot(ref_min_x, ref_min_y, \"g*\")\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    title = plt.title('it = 0')\n",
    "    \n",
    "    plots = []\n",
    "    for res_method in args:\n",
    "        plots.append(plt.plot(res_method[0, 0], res_method[0, 1], '-D', markevery=[-1])[0])\n",
    "    \n",
    "    def update(it):\n",
    "        for iplot, res_method in enumerate(args):\n",
    "            plots[iplot].set_data(res_method[:it+1, 0], res_method[:it+1, 1])\n",
    "            plots[iplot].set_markevery([it])\n",
    "        title.set_text(f\"it = {it}\")\n",
    "\n",
    "        return plots, title\n",
    "    \n",
    "    return FuncAnimation(fig, update, frames=nstep, interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed learning rate methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search\n",
    "\n",
    "This is a very naive approach.\n",
    "\n",
    "The algorithm:\n",
    "- chooses a random direction,\n",
    "- computes the coordinates of a test point from the current point in that random direction scaled by the learning rate,\n",
    "- checks the value of the loss function at this test point.\n",
    "\n",
    "If this value is less than the value of the loss function at the current point, the current point is updated with the test point.  \n",
    "If not, nothing happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search\n",
    "step = 0.1\n",
    "\n",
    "p_rand = np.empty((nstep, 2))\n",
    "p_rand[0, :] = start_point.copy()\n",
    "m = loss(*start_point)\n",
    "for n in range(1, nstep):\n",
    "    test_point = p_rand[n-1, :] + step * np.random.uniform(-1, 1, 2)\n",
    "    z = loss(*test_point)\n",
    "    if z < m:\n",
    "        m = z\n",
    "        p_rand[n, :] = test_point\n",
    "    else:\n",
    "        p_rand[n, :] = p_rand[n-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = plot_method(p_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "The classical gradient descent.  \n",
    "Step size (learning rate) is fixed, the direction is the opposite of the gradient (minimization).\n",
    "\n",
    "**Note** that, at the end, the gradient amplitude is low and the algorithm performs very small steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "step = 0.0005\n",
    "\n",
    "p_gd = np.empty((nstep, 2))\n",
    "p_gd[0, :] = start_point.copy()\n",
    "\n",
    "for n in range(1, nstep):\n",
    "    p_gd[n, :] = p_gd[n-1, :] - step * grad_loss(*p_gd[n-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = plot_method(p_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "The loss function is a sum of functions that are very similar.  \n",
    "Instead of computing the gradient of all functions in the sum (costly), only one function is used.  \n",
    "This function is chosen randomly at each iteration of the algorithm.\n",
    "\n",
    "Thus, this method is the same as the gradient descent, except that an approximation of the gradient is computed instead of the full gradient.  \n",
    "\n",
    "It suffers from the same problems as the gradient descent. The learning rate is fixed and the gradient amplitude is low which results in very small steps at the end of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "step = 0.0005\n",
    "\n",
    "p_sgd = np.empty((nstep, 2))\n",
    "p_sgd[0, :] = start_point.copy()\n",
    "\n",
    "for n in range(1, nstep):\n",
    "    batch = np.random.randint(0, N, 1)\n",
    "    p_sgd[n, :] = p_sgd[n-1, :] - step * grad_loss_batch(batch, *p_sgd[n-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = plot_method(p_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov's Accelerated Gradient (NAG)\n",
    "\n",
    "Published in 1983 by Yurii Nesterov.\n",
    "See also [Ilya Sutskever’s thesis](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf), section 7.2.\n",
    "\n",
    "This is based on the **momentum method**.\n",
    "\n",
    "### Momentum method\n",
    "We denote by $h$ the loss function. The algorithm iterates over the following operations:\n",
    "$$v_{n+1} = \\mu v_n - \\alpha \\nabla h(x_n) $$\n",
    "$$x_{n+1} = x_n + v_{n+1}$$\n",
    "where $v_0$ is initialized at $0$. The hyperparameter $\\mu \\in [0, 1]$ is the momentum constant (or $1-\\mu$ is a friction coefficient), and $\\alpha$ is the learning rate.\n",
    "\n",
    "Here, the gradient is used to update a velocity instead of the position directly.\n",
    "\n",
    "The constant $\\mu$ controls the \"decay\" of the velocity:\n",
    "a value close to $1$ means that the information of the previous gradients are kept for a longer time in the velocity.\n",
    "In contrast, a zero value means that the previous gradients are forgotten at each step, like in the classical gradient descent method.\n",
    "\n",
    "### NAG\n",
    "In the momentum method, the new velocity is computed using the gradient of the loss function at the previous position.  \n",
    "Instead of doing so, we use the information that the new point $x_{n+1}$ will be at the position $x_n + \\mu v_n + ...$.\n",
    "\n",
    "Thus, we can compute the gradient of the loss function at an ahead position: \n",
    "$$x_n^* = x_n + \\mu v_n$$\n",
    "\n",
    "The new algorithm writes:\n",
    "$$x_n^* = x_n + \\mu v_n$$\n",
    "$$v_{n+1} = \\mu v_n - \\alpha \\nabla h(x_n^*) $$\n",
    "$$x_{n+1} = x_n + v_{n+1}$$\n",
    "\n",
    "This can be re-written in terms of \"ahead position\" $x_n^*$:\n",
    "$$v_{n+1} = \\mu v_n - \\alpha \\nabla h(x_n^*) $$\n",
    "$$x_{n+1}^* = x_n^* - \\mu v_n + (1 + \\mu) v_{n+1}$$\n",
    "\n",
    "Which is what is implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nesterov momentum method\n",
    "# Can be SGD's\n",
    "step = 0.001\n",
    "mu = 0.95\n",
    "v = 0\n",
    "\n",
    "p_momentum = np.empty((nstep, 2))\n",
    "p_momentum[0, :] = start_point.copy()\n",
    "\n",
    "for n in range(1, nstep):\n",
    "    v_old = v\n",
    "    v = mu * v - step * grad_loss(*p_momentum[n-1, :])\n",
    "    p_momentum[n, :] = p_momentum[n-1, :] - mu * v_old + (1 + mu) * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = plot_method(p_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-parameter adaptive learning rate methods\n",
    "\n",
    "These methods adapt the learning rate for each parameter depending on the value of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "\n",
    "[Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochasticoptimization. *The Journal of Machine Learning Research*, 12:2121–2159, 2011](http://jmlr.org/papers/v12/duchi11a.html)\n",
    "\n",
    "The sum of the square of the gradient is kept in the variable `cache` in order to modify the learning rate. Note that it has the size of the gradient. Each parameter is modified with a different value (hence the \"per parameter\" keyword).\n",
    "\n",
    "The parameter $\\epsilon$ is an hyperparameter used to prevent division by $0$.\n",
    "\n",
    "The problem with this algorithm is that the learning rate can only decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "step = 0.5\n",
    "\n",
    "p_adagrad = np.empty((nstep, 2))\n",
    "p_adagrad[0, :] = start_point.copy()\n",
    "\n",
    "# hyperparameter\n",
    "eps = 1e-5\n",
    "\n",
    "cache = np.zeros((1, 2))\n",
    "for n in range(1, nstep):\n",
    "    grad = grad_loss(*p_adagrad[n-1, :])\n",
    "    cache += grad**2\n",
    "    p_adagrad[n, :] = p_adagrad[n-1, :] - (step / (np.sqrt(cache) + eps)) * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = plot_method(p_adagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    "\n",
    "Not published yet. https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "\n",
    "This algorithm is very similar to Adagrad.  \n",
    "It uses a moving average of squared gradients instead in order to reduce the aggressive, monotonically decreasing learning rate of Adagrad.  \n",
    "It introduces a new hyperparameter `decay_rate` that controls this moving averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop\n",
    "step = 0.1\n",
    "\n",
    "p_rmsprop = np.empty((nstep, 2))\n",
    "p_rmsprop[0, :] = start_point.copy()\n",
    "\n",
    "# hyperparameter\n",
    "eps = 1e-5\n",
    "decay_rate = 0.99\n",
    "\n",
    "cache = np.zeros((1, 2))\n",
    "for n in range(1, nstep):\n",
    "    grad = grad_loss(*p_rmsprop[n-1, :])\n",
    "    cache = decay_rate * cache + (1 - decay_rate) * grad**2\n",
    "    p_rmsprop[n, :] = p_rmsprop[n-1, :] - (step / (np.sqrt(cache) + eps)) * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anim = plot_method(p_rmsprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "[Diederik P. Kingma, Jimmy Ba. Adam: A Method For Stochastic Optimization. *Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015*](https://arxiv.org/abs/1412.6980v9)\n",
    "\n",
    "It can be seen as RMSprop + Momentum to smooth the gradient.  \n",
    "The simplified algorithm looks as follow:\n",
    "$$m_{n+1} = \\beta_1 m_n + (1 - \\beta_1) \\nabla h(x_n)$$\n",
    "\n",
    "$$v_{n+1} = \\beta_2 v_n + (1 - \\beta_2) \\nabla h(x_n)^2$$\n",
    "\n",
    "$$x_{n+1} = x_n - \\frac{\\alpha}{\\sqrt{v_{n+1}} + \\epsilon} m_{n+1}$$\n",
    "\n",
    "where $\\beta_1$ and $\\beta_2$ are two new hyperparameters, and $\\nabla h(x_n)^2$ denotes the point-wise squaring of $\\nabla h(x_n)$.\n",
    "\n",
    "The full Adam update also includes a bias correction mechanism, which compensates for the fact that in the first few iterations the vectors $m$ and $v$ are both initialized and therefore biased at zero, before they fully “warm up”:\n",
    "$$m_{n+1} = \\beta_1 m_n + (1 - \\beta_1) \\nabla h(x_n), \\qquad m_t = \\frac{m_{n+1}}{1 - \\beta_1^{n+1}}$$\n",
    "\n",
    "$$v_{n+1} = \\beta_2 v_n + (1 - \\beta_2) \\nabla h(x_n)^2, \\qquad v_t = \\frac{v_{n+1}}{1 - \\beta_2^{n+1}}$$\n",
    "\n",
    "$$x_{n+1} = x_n - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} m_t$$\n",
    "\n",
    "Note that in this new version the new position depends on the iteration number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam\n",
    "step = 0.1\n",
    "\n",
    "p_adam = np.empty((nstep, 2))\n",
    "p_adam[0, :] = start_point.copy()\n",
    "\n",
    "cache = np.zeros((1, 2))\n",
    "eps = 1e-8\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "m = 0\n",
    "v = 0\n",
    "for n in range(1, nstep):\n",
    "    grad = grad_loss(*p_adam[n-1, :])\n",
    "    \n",
    "    m = beta1 * m + (1 - beta1) * grad\n",
    "    mt = m / (1 - beta1**n)\n",
    "    v = beta2 * v + (1 - beta2) * grad**2\n",
    "    vt = v / (1 - beta2**n)\n",
    "    p_adam[n, :] = p_adam[n-1, :] - (step / (np.sqrt(vt) + eps)) * mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = plot_method(p_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scipy.optimize\n",
    "\n",
    "`scipy.optimize` has a function called `minimize` that minimizes scalar function of one or more variables. Different methods can be used, like BFGS methods.  \n",
    "It can be worth to give it a try.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFGS (scipy)\n",
    "import scipy.optimize as spo\n",
    "\n",
    "n = 0\n",
    "p_scipy = np.empty((nstep, 2))\n",
    "p_scipy[0, :] = start_point\n",
    "p_scipy[1:, 0] = ref_min_x\n",
    "p_scipy[1:, 1] = ref_min_y\n",
    "\n",
    "def f_scipy(x):\n",
    "    return loss(*x)\n",
    "\n",
    "def jac_f_scipy(x):\n",
    "    return grad_loss(*x)[0, :]\n",
    "\n",
    "def callback(xk):\n",
    "    global n\n",
    "    n += 1\n",
    "    p_scipy[n, :] = xk\n",
    "\n",
    "\n",
    "res = spo.minimize(f_scipy, start_point, method='BFGS', jac=jac_f_scipy, tol=-1e-15, \n",
    "                   options={\"maxiter\" : nstep, \"disp\" : False}, callback=callback)\n",
    "\n",
    "print(f\"Number of iteration: {res.nit}\")\n",
    "print(f\"Number of loss function evaluation: {res.nfev}\")\n",
    "print(f\"Number of gradient loss function evaluation: {res.njev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the number of evaluation of the loss function and its gradient.  \n",
    "This method does not scale as well as the other methods. It's fast for this toy problem but may not work well for large scale minimization problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = plot_method(p_scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = plot_method(p_rand, p_gd, p_sgd, p_momentum, p_adagrad, p_rmsprop, p_adam, p_scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
