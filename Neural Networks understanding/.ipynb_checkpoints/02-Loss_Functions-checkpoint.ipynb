{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these problems can be expressed as the **minimization** of a loss function $l : \\Omega \\to \\mathbb{R}$ where $\\Omega$ is the **parameters space**.\n",
    "\n",
    "Once we have the expression of the loss function, we can use an optimizer to find the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "For a regression problem, it is already formulated as a minimization problem:\n",
    "\n",
    "Given a point cloud $(x_k, y_k)$ in $\\mathbb{R}^N \\times \\mathbb{R}^M$, we define a model $m_{\\theta} : \\mathbb{R}^N \\to \\mathbb{R}^M$ that depends on the parameters $\\theta \\in \\Omega$ an that is used to fit this point cloud.\n",
    "\n",
    "An example of **loss function** is the squared l2-norm:\n",
    "$$\n",
    "J(\\mathbf{\\theta}) = \\frac{1}{2 K} \\sum_{k=1}^K \\left(m_{\\theta}(x_k) - y_k\\right)^2 .\n",
    "$$\n",
    "\n",
    "Thus we search for the best parameters $\\theta$ that minimize $J$:\n",
    "$$\n",
    "\\arg\\,\\min_{\\theta \\in \\Omega} J(\\theta) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "For a classification problem with $M$ classes, we can define a **score function** $s_{\\theta} : \\mathbb{R}^N \\to \\mathbb{R}^M$ that associates, for each input in $\\mathbb{R}^N$, a score to each class.\n",
    "\n",
    "From this score, the predicted class is the one with the highest score, so that the model $m_{\\theta} : \\mathbb{R}^N \\to [1, \\dots, M]$ can be expressed as:\n",
    "\n",
    "$$\n",
    "m_{\\theta}(x) = \\arg\\!\\max_{k \\in [1, \\dots, M]} s_{\\theta}(x)_k\n",
    "$$\n",
    "\n",
    "The associated **loss function** should be defined so that its gradient with respect to the parameters is not always zero, thus eliminating an expression based on $m_{\\theta}$.\n",
    "\n",
    "Many common loss functions exist, see:\n",
    "- http://cs231n.github.io/linear-classify/#loss,\n",
    "- https://indico.mathrice.fr/event/153/contribution/0/material/slides/0.pdf,\n",
    "- https://indico.mathrice.fr/event/153/contribution/1/material/1/0.pdf.\n",
    "\n",
    "The following examples of loss function are expressed for one sample of input/output. We can then take, for example, the mean for all samples to get the complete loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "For a given input $x_i$ with expected output $y_i$, SVM uses the following loss function $L_i$:\n",
    "\n",
    "$$\n",
    "L_i = \\sum_{j \\neq y_i} \\max(0, s_{\\theta}(x_i)_j - s_{\\theta}(x_i)_{y_i} + \\Delta)\n",
    "$$\n",
    "\n",
    "where $\\Delta$ is a positive margin between the score associated to the right class and all other classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax\n",
    "\n",
    "For a given input $x_i$ with expected output $y_i$, the SoftMax uses the following loss function $L_i$:\n",
    "\n",
    "$$\n",
    "L_i = - \\log \\left( \\frac{ \\exp(s_{\\theta}(x_i)_{y_i}) }{ \\sum_j \\exp(s_{\\theta}(x_i)_j)} \\right)\n",
    "$$\n",
    "\n",
    "where the score function is seen as a \"normalized probabilities\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "To get good optimized model properties, we may want to avoid high values of the parameters (it is out of scope of this course).\n",
    "For example, with the SVM loss function and given a $\\theta$ parameters set that minimizes the loss function, then any $\\alpha \\theta$ parameters scaled with $\\alpha >= 1$ is also a minimizer.\n",
    "\n",
    "In that case, a regularization term can be added in the loss function that penalizes high values of the parameters, e.g.:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{I} \\sum_i^I L_i + \\lambda \\frac{1}{K} \\sum_k^K \\theta_k^2\n",
    "$$\n",
    "for $I$ samples and $K$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
