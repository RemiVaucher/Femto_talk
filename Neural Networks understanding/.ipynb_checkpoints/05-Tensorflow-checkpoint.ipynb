{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "## Construction\n",
    "\n",
    "A tensor is a **Nd-array** like in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [[1. 2.]\n",
      " [3. 4.]]\n",
      "b = [5. 6.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1., 2.], [3., 4.]])\n",
    "print(f\"a = {a}\")\n",
    "\n",
    "b = tf.constant([5., 6.])\n",
    "print(f\"b = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative tensor **construction** exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1 = [[ 1.33359685  0.13459115 -1.1691243   0.18322556]\n",
      " [ 0.45117772  1.00664955 -1.00284915  0.84536542]\n",
      " [-0.01814427 -0.34787968 -0.30603083  1.59084713]]\n",
      "\n",
      "e2 = [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "\n",
      "e3 = [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e1 = tf.constant(np.random.normal(0, 1, (3, 4)))\n",
    "print(f\"e1 = {e1}\\n\")\n",
    "\n",
    "e2 = tf.ones((3, 4))\n",
    "print(f\"e2 = {e2}\\n\")\n",
    "\n",
    "e3 = tf.zeros((3, 4))\n",
    "print(f\"e3 = {e3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in numpy, you can specify the **data type** (`int`, `float`, `double`, ...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = tf.ones((3, 4), dtype=tf.float32) / 3\n",
    "print(f\"float32: {e1[0, 0]}\\n\")\n",
    "\n",
    "e2 = tf.ones((3, 4), dtype=tf.float64) / 3\n",
    "print(f\"float64: {e2[0, 0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "Like in numpy, many operations are defined.\n",
    "\n",
    "However, you must only use **Tensorflow functions** (no Numpy!) in order to benefit from the automatic differentiation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0, 1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting\n",
    "c = a + b\n",
    "print(f\"c = {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-tensor\n",
    "d = c[0, :]\n",
    "print(f\"d = {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other functions that return a new tensor\n",
    "print(tf.reduce_max(c))\n",
    "print(tf.sin(c))\n",
    "print(tf.reduce_sum(c**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More operations are available in **tf.math** https://www.tensorflow.org/api_docs/python/tf/math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To and from Numpy\n",
    "\n",
    "You can easily get a `tf.Tensor` from a numpy array and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(3, 2)\n",
    "b = tf.convert_to_tensor(a)\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b.numpy()\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"c = {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "A **feed-forward** neural network can be seen as a function $f_\\theta: \\mathbb{R}^N \\to \\mathbb{R}^M$ that is the composition of layers of two kinds:\n",
    "\n",
    "- **affine layers** (also called \"linear\" layers).\n",
    "  A typical exemple is $L(x) = A x + b$ with $A$ a matrix of appropriate size.\n",
    "  We can also consider a convolution layer $C(x) = x \\star k$ with $k$ the kernel.\n",
    "- **non-linear layers**.\n",
    "  Typical non-linear layers are called *activation layer* like the function $ReLU(x) = \\max(0, x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we could have:\n",
    "\n",
    "$$f = L_k \\circ ReLU \\circ L_{k-1} \\circ ReLU \\circ \\dots \\circ ReLU \\circ L_1$$\n",
    "\n",
    "Here, if $L_i(x) = A_i x + b_i$, the **parameters** $\\theta$ of the model are all the coefficients of $A_i$ and $b_i$.\n",
    "\n",
    "**Note that:**\n",
    "- using two **consecutive linear layers** is equivalent to one linear layer,\n",
    "- using a non-linear layer as **last layer** is uncommon since it bounds the output values,\n",
    "- we typically count only the linear layers, so that:\n",
    "  - the identity function (inputs = outputs) is called a 0-layer neural network,\n",
    "  - the function $f_\\theta = L$ composed of only one linear layer is called a 1-layer neural network,\n",
    "  - the function $f = L_k \\circ ReLU \\circ L_{k-1} \\circ ReLU \\circ \\dots \\circ ReLU \\circ L_1$ is called a k-layers neural network.\n",
    "- many **activation layers** exist with different properties, see http://cs231n.github.io/neural-networks-1/#actfun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "### Common layers\n",
    "\n",
    "Tensorflow features many kinds of layers in the `tf.keras.layers` namespace (see https://www.tensorflow.org/api_docs/python/tf/keras/layers/).\n",
    "\n",
    "They are implemented as classes and you can create custom layers by defining a class that inherits from `tf.keras.layers.Layer` (see https://www.tensorflow.org/tutorials/customization/custom_layers)\n",
    "\n",
    "Some common layers are:\n",
    "- linear (or **dense**) layer in [`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) that model the $Ax + b$ function with $A \\in \\mathcal{M}_{M, N}(\\mathbb{R})$ a dense matrix and $b \\in \\mathbb{R}^M$ the optional bias.\n",
    "- activation layers in https://www.tensorflow.org/api_docs/python/tf/keras/activations like \n",
    "  [`tf.keras.activations.relu`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu), \n",
    "  [`tf.keras.activations.elu`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu), \n",
    "  [`tf.keras.activations.tanh`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh),\n",
    "  [`tf.keras.activations.sigmoid`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid),..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very simple model (a perceptron)\n",
    "layer = tf.keras.layers.Dense(2, input_shape=(None, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply it, simply call it\n",
    "layer(tf.zeros((1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying its parameters (matrix and bias)\n",
    "print(layer.kernel)\n",
    "print(layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of layers\n",
    "\n",
    "When your model is only a sequence of layers, you can simply use a `tf.keras.Sequential` module with all layers as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model with two dense layer, and a ReLU activation after the first one\n",
    "N, M = 10, 2\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(N // 2, input_dim=N, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(M, activation='linear'))\n",
    "\n",
    "model.summary()\n",
    "print()\n",
    "\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(f\"Parameters for layer {i}\")\n",
    "    print(layer.kernel)\n",
    "    print(layer.bias)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the model on a 3 random inputs\n",
    "x = tf.random.normal((3, N))\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom activation function\n",
    "\n",
    "You can create custom activation function. They are simply tensorflow functions that take tensor as inputs and return a tensor of same shape and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian-shaped activation function\n",
    "def gaussian_activation(t):\n",
    "    return tf.math.exp(-t**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Dense(N // 2, input_dim=N, activation=gaussian_activation))\n",
    "model2.add(tf.keras.layers.Dense(M, activation='linear'))\n",
    "\n",
    "# Applying the model on the same 3 inputs\n",
    "y = model2(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "Some loss functions are available in `tf.keras.losses` (see https://www.tensorflow.org/api_docs/python/tf/keras/losses):\n",
    "- [`tf.keras.losses.MeanSquareError`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError) for the l2-norm,\n",
    "- [`tf.keras.losses.SparseCategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) for a loss function adapted to classification problems.\n",
    "\n",
    "Given a model (a neural network) that depends on some parameters, we want to find values of these parameters such that the result of the model apply to some data are close to what we want.\n",
    "This \"distance\" is given by the loss function. We want to minimize the loss function with respect to the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "Tensorflow features many optimization algorithms in the `tf.optimizers` module (see the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)).\n",
    "\n",
    "All these optimizers have a common interface:\n",
    "1. first, define your model and its **parameters**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# N Rosenbrock functions\n",
    "N = 1000\n",
    "\n",
    "# The two parameters for each Rosenbrock function\n",
    "a = 0.1 * tf.random.normal((N,), dtype=tf.float64) + 1\n",
    "b = 1 * tf.random.normal((N,), dtype=tf.float64) + 10\n",
    "\n",
    "# This is the minimum of the sum of the Rosenbrock functions\n",
    "ref_min_x = tf.math.reduce_mean(a)\n",
    "ref_min_y = ref_min_x**2\n",
    "\n",
    "# The Rosenbrock function\n",
    "def f(a, b, x, y):\n",
    "    return (x - a)**2 + b * (x**2 - y)**2\n",
    "\n",
    "\n",
    "# Loss function, the one we want to minimize\n",
    "# Mean of Rosenbrock functions\n",
    "def loss(x, y):\n",
    "    return tf.math.reduce_mean(f(a, b, x, y), axis=-1)\n",
    "\n",
    "\n",
    "# The variables to update during the minimization of the loss function.\n",
    "# They are initialized with the start point for the minimization\n",
    "x = tf.Variable(0.1, dtype=tf.float64)\n",
    "y = tf.Variable(2.5, dtype=tf.float64)\n",
    "\n",
    "print(f\"loss({x.numpy()}, {y.numpy()}) = {loss(x.numpy(), y.numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create the optimizer and set some hyper-parameters like the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.SGD(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Minimize the loss by calling the function `minimize` of the optimizer. This function performs only one step of the optimizer, you need to call it multiple time to find the minimum. Input arguments are:\n",
    "\n",
    "- a loss function that takes no arguments.\n",
    "- the variables to update during minimization. The gradient of the loss function will be computed with respect to these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstep = 1000\n",
    "for _ in range(nstep):\n",
    "    step_count = optimizer.minimize(lambda : loss(x, y), [x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of steps = {step_count.numpy()}\")\n",
    "print(f\"x = {x.numpy()}\")\n",
    "print(f\"y = {y.numpy()}\")\n",
    "print(f\"loss({x.numpy()}, {y.numpy()}) = {loss(x.numpy(), y.numpy())}\")\n",
    "\n",
    "print(f\"min_x = {ref_min_x}\")\n",
    "print(f\"min_y = {ref_min_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Optimizers list and parameters\n",
    "optimizers = [\n",
    "    {'name': 'Adadelta', 'class': tf.optimizers.Adadelta, 'args': dict(learning_rate=1e-1, rho=0.95)},\n",
    "    {'name': 'Adagrad', 'class': tf.optimizers.Adagrad, 'args': dict(learning_rate=0.5)},\n",
    "    {'name': 'Adam', 'class': tf.optimizers.Adam, 'args': dict(learning_rate=0.1, beta_1=0.9, beta_2=0.999)},\n",
    "    {'name': 'Adamax', 'class': tf.optimizers.Adamax, 'args': dict(learning_rate=0.1, beta_1=0.9, beta_2=0.999)},\n",
    "    {'name': 'SGD', 'class': tf.optimizers.SGD, 'args': dict(learning_rate=0.0005)},\n",
    "]\n",
    "\n",
    "# The start point for the minimization\n",
    "start_point = tf.constant([0.1, 2.5], dtype=tf.float64)\n",
    "\n",
    "# nstep for each algorithms (easier to compare)\n",
    "nstep = 300\n",
    "\n",
    "# Iteration points for each optimizers\n",
    "points = np.empty((len(optimizers), nstep, 2))\n",
    "\n",
    "for config_id, config in enumerate(optimizers):\n",
    "    # The start point for the minimization\n",
    "    pt = tf.Variable(start_point)\n",
    "    points[config_id, 0, :] = pt.numpy()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = config['class'](**config['args'])\n",
    "\n",
    "    # Optimizing\n",
    "    for i in range(1, nstep):\n",
    "        optimizer.minimize(lambda: loss(pt[0], pt[1]), [pt])\n",
    "        points[config_id, i, :] = pt.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import cm\n",
    "\n",
    "# Function to play the animation of the results\n",
    "def plot_method(optimizers, points):\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    plt.contour(X, Y, Z, levels=levels, cmap=cm.coolwarm, vmin=np.amin(Z), vmax=np.amax(Z), alpha=0.5)\n",
    "    plt.plot(start_point[0], start_point[1], \"rX\")\n",
    "    plt.plot(ref_min_x, ref_min_y, \"g*\")\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    title = plt.title('it = 0')\n",
    "    \n",
    "    plots = []\n",
    "    for config_id, config in enumerate(optimizers):\n",
    "        plots.append(plt.plot(points[config_id, 0, 0], points[config_id, 0, 1], '-D', markevery=[-1], label=config['name'])[0])\n",
    "    plt.legend()\n",
    "    \n",
    "    def update(it):\n",
    "        for config_id in range(len(optimizers)):\n",
    "            plots[config_id].set_data(points[config_id, :it+1, 0], points[config_id, :it+1, 1])\n",
    "            plots[config_id].set_markevery([it])\n",
    "        title.set_text(f\"it = {it}\")\n",
    "            \n",
    "        return plots\n",
    "    \n",
    "    return FuncAnimation(fig, update, frames=nstep, interval=10)\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-2, 2, 0.1)\n",
    "Y = np.arange(-1, 3, 0.1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = loss(X[:, :, None], Y[:, :, None])\n",
    "levels = [1e-5, 1e-1, 1, 10, 50, 100, 200]\n",
    "\n",
    "anim = plot_method(optimizers, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Tensorflow provides datasets to feed your neural network with data. They are generators, like the `range` function, and many operations can be apply easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation\n",
    "\n",
    "The simplest way to create a dataset is from a tensor, numpy array or python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a numpy array\n",
    "npdata = np.arange(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(npdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can then iterate over the dataset\n",
    "for data in dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data doesn't fit in memory, you can write them in `.tfrecord` files and create a dataset from the files with the function `tf.data.TFRecordDataset`. We will not look into it here, but when dealing with a huge amount of data, this is the only solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training, you will need training data and the exact results (labels) that your neural network will approximate\n",
    "# This can be done by passing a tuple of the two arrays.\n",
    "training_data = np.arange(10)\n",
    "labels = training_data**2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((training_data, labels))\n",
    "\n",
    "for data, label in dataset:\n",
    "    print(data, \" -- \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation and preparation of a dataset for training\n",
    "\n",
    "Many operation can be apply on a dataset before using it in training phase. One can apply transformations on the data, cache them (store them in memory for faster access), batch them, prefetch them and shuffle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a transformation : use the map function of dataset\n",
    "npdata = np.arange(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(npdata)\n",
    "\n",
    "dataset = dataset.map(lambda x: x**2, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "for data in dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only the first n elements\n",
    "for data in dataset.take(3):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching data for faster access is done with the cache function\n",
    "# The difference is visible during training phase when we iterate many times over the dataset\n",
    "dataset = dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling : data in dataset are shuffled. The only argument is the number of data in the shuffle buffer.\n",
    "for data in dataset.shuffle(2):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset fills a random buffer with 2 elements and randomly pick one. \n",
    "# The taken data is then replace with another element.\n",
    "# For perfect shuffling of all the data in the dataset, a buffer size greater or equal to the number of data is\n",
    "# mandatory.\n",
    "for data in dataset.shuffle(10):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can batch samples from the dataset to take more than one at once\n",
    "# Here we want the data by batch of 2\n",
    "for data in dataset.shuffle(10).batch(2):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During training phase, for faster use of dataset, you can prefetch the data.\n",
    "# It means that every operations that come before the prefetch are perform at the same time as the training\n",
    "# Here we prefetch 2 shuffled batches of size 2 while printing the previous two batches\n",
    "for data in dataset.shuffle(10).batch(2).prefetch(2):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "201.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
