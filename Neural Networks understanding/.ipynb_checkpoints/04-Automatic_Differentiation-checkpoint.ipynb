{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use a gradient type optimization method as seen previously, \n",
    "we need to be able to calculate the **gradient** of the function.\n",
    "\n",
    "We don't want to specify the gradient expression each time we define a new function:\n",
    "- it is a hard work,\n",
    "- it is heavily error prone,\n",
    "- it is a repetition (the function definition already \"contains\" its gradient expression).\n",
    "\n",
    "**Automatic differentiation** algorithms are able to recover from the implementation of a function, its gradient relatively to any of its input parameters at a given evaluation point.\n",
    "\n",
    "E.g., given a $f: \\mathbb{R}^N \\to \\mathbb{R}^M$ function and $x \\in \\mathbb{R}^N$, such kind of algorithm should be able to return $Df(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite difference schemes\n",
    "\n",
    "The finite difference schemes can be seen as a first example of automatic differentiation.\n",
    "\n",
    "For a given function $f: \\mathbb{R}^N \\to \\mathbb{R}$, we can approximate its differential by using a finite difference scheme, like:\n",
    "\n",
    "$$\\forall i = 1, ..., N \\quad \\frac{\\partial f}{\\partial x_i}(x_1, ..., x_N) = \\frac{f(x_1, ..., x_i+h, ..., x_N) - f(x_1, ..., x_i, ..., x_N)}{h} + \\mathcal{O}(h)$$\n",
    "\n",
    "for the **1st-order** version and:\n",
    "\n",
    "$$\\forall i = 1, ..., N \\quad \\frac{\\partial f}{\\partial x_i}(x_1, ..., x_N) = \\frac{f(x_1, ..., x_i+h, ..., x_N) - f(x_1, ..., x_i-h, ..., x_N)}{2h} + \\mathcal{O}(h^2)$$\n",
    "\n",
    "for the **2nd-order** centered version.\n",
    "\n",
    "**Pro:**\n",
    "- doesn't need to know how to calculate $f$ (black box).\n",
    "\n",
    "**Cons:**\n",
    "- $N+1$ evaluation for the 1st order scheme and $2N$ for the 2nd order version,\n",
    "- how do we choose $h$ ? The big-O term also depends on the second-order derivative of $f$..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: a dummy function\n",
    "\n",
    "To illustrate the automatic differentiation process, we use the following function $f : \\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n",
    "\n",
    "$$x, y \\longmapsto \\frac{x + \\max(x, y)}{y + (x+y)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2, 1) = 0.4\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return (x + max(x, y)) / (y + (x + y)**2)\n",
    "\n",
    "x = 2\n",
    "y = 1\n",
    "\n",
    "print(f\"f({x}, {y}) = {f(x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2nd-order finite difference scheme to approximate its derivative:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x}(x, y) = \\frac{f(x+h, y) - f(x-h, y)}{2h} + \\mathcal{O}(h^2)$$\n",
    "$$\\frac{\\partial f}{\\partial y}(x, y) = \\frac{f(x, y+h) - f(x, y+h)}{2h} + \\mathcal{O}(h^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h =      1 ; dfdx = -0.02352941176470588 ; dfdy = -0.3888888888888889\n",
      "h =    0.1 ; dfdx = -0.03986374212365584 ; dfdy = -0.2808140800179723\n",
      "h =   0.01 ; dfdx = -0.03999863997423692 ; dfdy = -0.2800081202074811\n",
      "h =  0.001 ; dfdx = -0.03999998640000224 ; dfdy = -0.2800000811999936\n",
      "h = 0.0001 ; dfdx = -0.03999999986348257 ; dfdy = -0.2800000008112979\n",
      "h =  1e-05 ; dfdx = -0.03999999999837467 ; dfdy = -0.2800000000080516\n",
      "h =  1e-06 ; dfdx = -0.04000000003445692 ; dfdy = -0.2800000000191538\n",
      "h =  1e-07 ; dfdx = -0.03999999997894577 ; dfdy = -0.2800000001301761\n",
      "h =  1e-08 ; dfdx = -0.04000000053405728 ; dfdy = -0.2799999981872858\n",
      "h =  1e-09 ; dfdx = -0.04000000330961484 ; dfdy = -0.2800000231673039\n",
      "h =  1e-10 ; dfdx = -0.03999994779846361 ; dfdy = -0.2800001897007576\n",
      "h =  1e-11 ; dfdx = -0.03999856001968283 ; dfdy = -0.2799954712529029\n",
      "h =  1e-12 ; dfdx = -0.03999578446212126 ; dfdy = -0.2800260023860801\n",
      "h =  1e-13 ; dfdx = -0.03969047313034935 ; dfdy = -0.2792210906932269\n",
      "h =  1e-14 ; dfdx = -0.03885780586188048 ; dfdy = -0.2775557561562891\n",
      "h =  1e-15 ; dfdx = 0.0 ; dfdy = -0.305311331771918\n"
     ]
    }
   ],
   "source": [
    "for p in range(16):\n",
    "    h = 10**(-p)\n",
    "    dfdx = (f(x+h, y) - f(x-h, y))/(2*h)\n",
    "    dfdy = (f(x, y+h) - f(x, y-h))/(2*h)\n",
    "    print(f\"h = {h:6} ; dfdx = {dfdx:.16} ; dfdy = {dfdy:.16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** you can observe a classical loss of precision in the finite difference method for small space steps (starting at `1e-6`) due to a catastrophic cancellation effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The chain rule\n",
    "\n",
    "Given the functions:\n",
    "- $f: \\mathbb{R}^C \\to \\mathbb{R}^D$,\n",
    "- $g: \\mathbb{R}^B \\to \\mathbb{R}^C$,\n",
    "- $h: \\mathbb{R}^A \\to \\mathbb{R}^B$,\n",
    "\n",
    "we want to compute the derivative of $f \\circ g \\circ h$ at a given point $x \\in \\mathbb{R}^A$.\n",
    "\n",
    "Using the **chain rule**, we get the following expression of the Jacobians $J$:\n",
    "\n",
    "$$J_{f \\circ g \\circ h}(x) = J_f(g(h(x))) \\cdot J_g(h(x)) \\cdot J_h(x)$$\n",
    "\n",
    "where\n",
    "- $J_f (g(h(x))) \\in \\mathcal{M}_{D, C}$,\n",
    "- $J_g (h(x)) \\in \\mathcal{M}_{C, B}$ and\n",
    "- $J_h (x) \\in \\mathcal{M}_{B, A}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **which way** do we evaluate this matrix product :\n",
    "\n",
    "1. From left to right:\n",
    "$$J_{f \\circ g \\circ h}(x) = \\underbrace{\\underbrace{\\underbrace{J_f(g(h(x)))}_1 \\cdot J_g(h(x))}_2 \\cdot J_h(x)}_3$$\n",
    "\n",
    "2. From right to left:\n",
    "    $$J_{f \\circ g \\circ h}(x) = \\underbrace{J_f(g(h(x))) \\cdot \\underbrace{J_g(h(x)) \\cdot \\underbrace{J_h(x)}_1}_2}_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function evaluation: the forward way\n",
    "\n",
    "We call **forward** way, the order of evaluation when calculating $f(g(h(x))$:\n",
    "\n",
    "1. $x_1 = x$,\n",
    "2. $x_2 = h(x_1)$,\n",
    "3. $x_3 = g(x_2)$,\n",
    "4. $x_4 = f(x_3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient evaluation: forward accumulation\n",
    "\n",
    "Following the same order to evaluate the Jacobian:\n",
    "1. $M_1 = J_h(x_1) \\quad\\quad\\quad \\in \\mathcal{M}_{B, A}$,\n",
    "2. $M_2 = J_g(x_2) \\cdot M_1 \\quad \\in \\mathcal{M}_{C, A}$,\n",
    "3. $M_3 = J_f(x_3) \\cdot M_2 \\quad \\in \\mathcal{M}_{D, A}$.\n",
    "\n",
    "$J_{f \\circ g \\circ h}(x) = M_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient evaluation: reverse accumulation\n",
    "Or **backpropagation**.\n",
    "\n",
    "1. $M_1 = J_f(x_3) \\quad\\quad\\quad \\in \\mathcal{M}_{D, C}$,\n",
    "2. $M_2 = M_1 \\cdot J_g(x_2) \\quad \\in \\mathcal{M}_{D, B}$,\n",
    "3. $M_3 = M_2 \\cdot J_h(x_1) \\quad \\in \\mathcal{M}_{D, A}$.\n",
    "\n",
    "$J_{f \\circ g \\circ h}(x) = M_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And for a loss function ?!!\n",
    "\n",
    "In a machine learning process, we want to differentiate a **loss function** (a $\\mathbb{R}^N \\to \\mathbb{R}$ function): we have $D = 1$ and other dimensions can be pretty high.\n",
    "\n",
    "The **forward accumulation** expresses as:\n",
    "1. $M_1 = J_h(x_1) \\quad\\quad\\quad \\in \\mathcal{M}_{B, A}$,\n",
    "2. $M_2 = J_g(x_2) \\cdot M_1 \\quad \\in \\mathcal{M}_{C, A}$,\n",
    "3. $M_3 = J_f(x_3) \\cdot M_2 \\quad \\in \\mathcal{M}_{1, A}$.\n",
    "\n",
    "The **reverse accumulation** expresses as:\n",
    "1. $M_1 = J_f(x_3) \\quad\\quad\\quad \\in \\mathcal{M}_{1, C}$,\n",
    "2. $M_2 = M_1 \\cdot J_g(x_2) \\quad \\in \\mathcal{M}_{1, B}$,\n",
    "3. $M_3 = M_2 \\cdot J_h(x_1) \\quad \\in \\mathcal{M}_{1, A}$.\n",
    "\n",
    "Supposing we can do the matrix product with the Jacobian without storing the Jacobian matrix in memory, we get the following **complexities**:\n",
    "- **forward** accumulation: about $\\mathcal{O}(AB + ABC + BC)$ in computational cost and $\\mathcal{O}(\\max(AB, AC, A))$ in memory cost.\n",
    "- **reverse** accumulation: about $\\mathcal{O}(C + BC + AB)$ in computational cost and $\\mathcal{O}(\\max(A, B, C))$ in memory cost.\n",
    "\n",
    "$\\Longrightarrow$ **reverse accumulation** (**backpropagation**)!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: the dummy function\n",
    "\n",
    "We recall our example function $f : \\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n",
    "\n",
    "$$x, y \\longmapsto \\frac{x + \\max(x, y)}{y + (x+y)^2}$$\n",
    "\n",
    "We first rewrite it as a composition of basics functions:\n",
    "\n",
    "$$ f(x, y) = \\mathrm{div}(\\mathrm{add}(x, \\mathrm{max}(x, y)), \\mathrm{add}(y, \\mathrm{sqr}(\\mathrm{add}(x, y))) $$\n",
    "\n",
    "with the following function expressions and derivatives:\n",
    "- $\\mathrm{add}(a, b) = a + b$ with derivatives $\\partial_a \\mathrm{add}(a,b) = \\partial_b \\mathrm{add}(a,b) = 1$,\n",
    "- $\\mathrm{sqr}(a) = a^2$ with derivative $\\partial_a \\mathrm{sqr}(a) = 2a$,\n",
    "- $\\mathrm{div}(a, b) = \\frac{a}{b}$ with derivatives $\\partial_a \\mathrm{div}(a,b) = \\frac{1}{b}$ and $\\partial_b \\mathrm{div}(a,b) = - \\frac{a}{b^2}$,\n",
    "- $\\mathrm{max}(a, b)$ with derivatives $\\partial_a \\mathrm{max}(a, b) = \\mathbb{1}(a >= b)$ and $\\partial_b \\mathrm{max}(a, b) = \\mathbb{1}(b >= a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "We first evaluate the function during the forward pass with $x = 2$ and $y = 1$:\n",
    "\n",
    "1. $a_1 = x = 2$,  $b_1 = y = 1$, $v_1 = \\mathrm{max}(a_1, b_1) = 2$\n",
    "2. $a_2 = x = 2$, $b_2 = v_1 = 2$, $v_2 = \\mathrm{add}(a_2, b_2) = 4$\n",
    "3. $a_3 = x = 2$, $b_3 = y = 1$, $v_3 = \\mathrm{add}(a_3, b_3) = 3$\n",
    "4. $a_4 = v_3 = 3$, $v_4 = \\mathrm{sqr}(a_4) = 9$\n",
    "5. $a_5 = y = 1$, $b_5 = v_4 = 9$, $v_5 = \\mathrm{add}(a_5, b_5) = 10$\n",
    "6. $a_6 = v_2 = 4$, $b_6 = v_5 = 10$, $v_6 = \\mathrm{div}(a_6, b_6) = 0.4$\n",
    "\n",
    "Thus, we recover the result $f(2,1) = 0.4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "Now, we rollback the previous forward pass to calculate the derivatives $f(x,y)$ at $x=2$ and $y=1$:\n",
    "\n",
    "1. Step 6: $da_6 = \\partial_a \\mathrm{div}(a_6, b_6) = \\frac{1}{b_6} = 0.1$ and\n",
    "   $db_6 = \\partial_b \\mathrm{div}(a_6, b_6) = - \\frac{a_6}{b_6^2} = -0.04$\n",
    "2. Step 5: $da_5 = 1 \\times dv_5 = db_6 = -0.04$ and\n",
    "   $db_5 = 1 \\times dv_5 = db_6 = -0.04$\n",
    "3. Step 4: $da_4 = 2 a_4 \\times dv_4 = 2 a_4 db_5 = -0.24$\n",
    "4. Step 3: $da_3 = 1 \\times dv_3 = da_4 = -0.24$ and\n",
    "   $db_3 = 1 \\times dv_3 = da_4 = -0.24$\n",
    "5. Step 2: $da_2 = 1 \\times dv_2 = da_6 = 0.1$ and\n",
    "   $db_2 = 1 \\times dv_2 = da_6 = 0.1$\n",
    "6. Step 1: $da_1 = \\mathbb{1}(a_1 >= b_1) \\times dv_1 = db_2 = 0.1$ and\n",
    "   $db_1 = \\mathbb{1}(b_1 >= a_1) \\times dv_1 = 0$\n",
    "\n",
    "Finally, we get the derivative of $f$ with respect to $x$ and $y$ by summing the contributions to $dx$ and $dy$:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x}(2, 1) = da_1 + da_2 + da_3 = 0.1 + 0.1 - 0.24 = -0.04$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial y}(2, 1) = db_1 + db_3 + da_5 = 0 - 0.24 - 0.04 = -0.28$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return (x + max(x, y)) / (y + (x + y)**2)\n",
    "\n",
    "x = 2\n",
    "y = 1\n",
    "\n",
    "# Forward pass\n",
    "a1 = x; b1 = y;  v1 = max(a1, b1)\n",
    "a2 = x; b2 = v1; v2 = a2 + b2\n",
    "a3 = x; b3 = y; v3 = a3 + b3\n",
    "a4 = v3; v4 = a4**2\n",
    "a5 = y; b5 = v4; v5 = a5 + b5\n",
    "a6 = v2; b6 = v5; v6 = a6 / b6\n",
    "print(f\"f({x}, {y}) = {v6}\")\n",
    "\n",
    "# Backward pass\n",
    "da6 = 1 / b6; db6 = -a6 / b6**2\n",
    "da5 = db6; db5 = db6\n",
    "da4 = 2 * a4 * db5\n",
    "da3 = da4; db3 = da4\n",
    "da2 = da6; db2 = da6\n",
    "da1 = (a1 >= b1) * db2; db1 = (b1 >= a1) * db2 # Booleans are convertible to int (False <=> 0, True <=> 1)\n",
    "dfdx = da1 + da2 + da3; dfdy = db1 + db3 + da5\n",
    "print(f\"Df({x}, {y}) = ({dfdx}, {dfdy})\")\n",
    "\n",
    "# Gradient checking using finite differences\n",
    "def grad_check(f, x, y, h):\n",
    "    return ((f(x+h, y) - f(x-h, y)) / (2*h), \n",
    "            (f(x, y+h) - f(x, y-h)) / (2*h))\n",
    "    \n",
    "dfdx_fd2, dfdy_fd2 = grad_check(f, x, y, 1e-8)\n",
    "print(f\"Grad check: Df({x}, {y}) = ({dfdx_fd2}, {dfdy_fd2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
